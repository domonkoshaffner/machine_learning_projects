{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "315DLxAdHjPC"
   },
   "source": [
    "# HW 11.\n",
    "\n",
    "* Running the models may take minutes. This HW takes ~30 min to complete in computational time, so make sure you don't start it 1 hour before it is due.\n",
    "\n",
    "* Tasks 2-4. should be done using the `sklearn` library, the last is a pure TensorFlow ([Keras is part of TensorFlow](https://github.com/keras-team/keras/releases#:~:text=since%20this%20release-,Keras%202.2.,well%20as%20Theano%20and%20CNTK)) example.\n",
    "\n",
    "  * Use tf.keras instead of the standalone keras package\n",
    "\n",
    "* The example notebook was run in Google COLAB without any package installation. I advise you to use Google COLAB with a GPU instance for the last task.\n",
    "\n",
    "* Where not asked otherwise, use the default settings for the model.\n",
    "\n",
    "* You may try running the models using more CPU cores to speed the training (sklearn supports for most of the models with a parameter, usually n_jobs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "djQxSQ3Z3ZCr"
   },
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow import keras\n",
    "import tensorflow\n",
    "\n",
    "import random\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, auc as auc_score, roc_curve, accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w0FrNEnb3ZCs"
   },
   "source": [
    "### 1. Load the CIFAR 10 dataset from the `tf.keras.datasets` API and train a `LogisticRegression` model on the dataset and predict all test outcomes with the `sklearn` API\n",
    "\n",
    "* Create an image grid visualization of randomly selected images (9, 16) with labels.\n",
    "* Preprocess the dataset for `sklearn`, scale the pixels [0-1], and also flatten each example to a vector.\n",
    "* Use the `multi_class='multinomial'` option, describe what it means.\n",
    "* Plot the ROC curves and AUC scores on the same figure for each class.\n",
    "* Calculate the accuracy of the classifier on the test set.\n",
    "\n",
    "\n",
    "Hint:\n",
    "\n",
    "* `from sklearn.preprocessing import LabelBinarizer` might be useful for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "xG_Q9bw73ZCs",
    "outputId": "6feff6b6-2ca7-4f0b-d45a-748cf0c0379c"
   },
   "outputs": [],
   "source": [
    "#loading the dataset\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "#printing the shapes \n",
    "\n",
    "print('The shape of the train dataset: ', np.shape(x_train))\n",
    "print('The shape of the test dataset: ', np.shape(x_test))\n",
    "print()\n",
    "\n",
    "#creating a plot of the first 10 pictures\n",
    "\n",
    "fig, axs = plt.subplots(2,5, figsize=(15, 6), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(10):\n",
    "\n",
    "    axs[i].imshow(x_train[i])\n",
    "    axs[i].set_title('The label: ' + str(y_train[i]))\n",
    "    axs[i].axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z817qfHuE8Aj"
   },
   "source": [
    "After checking the labels online, we can plot the pictures with the actual labels (not numbers):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_3Dy5EcFB3L"
   },
   "outputs": [],
   "source": [
    "#putting the actual labels in a list\n",
    "\n",
    "actual_labels = ['airplane', 'automobile', 'bird',\t'cat', 'deer', 'dog',\t'frog',\t'horse', 'ship','truck']\n",
    "\n",
    "#creating some neew lists with the actual labels\n",
    "\n",
    "actual_label_list_train = []\n",
    "actual_label_list_test = []\n",
    "\n",
    "for i in range(0,len(y_train)):\n",
    "  actual_label_list_train.append(actual_labels[y_train[i][0]])\n",
    "\n",
    "for i in range(0,len(y_test)):\n",
    "  actual_label_list_test.append(actual_labels[y_test[i][0]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ijHkbXdHPze"
   },
   "outputs": [],
   "source": [
    "#generating random indices to plot\n",
    "\n",
    "random_indices = []\n",
    "\n",
    "for i in range(0,144):\n",
    "  random_indices.append(random.randint(0,50000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mERGvrg3ZCs"
   },
   "source": [
    "The shapes mean that there are 50000 training- and 10000 testing images, all in RGB. Let's preprocess the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 396
    },
    "id": "EQTviCbuI2j6",
    "outputId": "54706f58-9123-458e-d9dd-869b20bb1ff1"
   },
   "outputs": [],
   "source": [
    "#creating a plot of the first 10 pictures\n",
    "\n",
    "fig, axs = plt.subplots(5,10, figsize=(35, 10), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = .5, wspace=.001)\n",
    "\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(50):\n",
    "\n",
    "    axs[i].imshow(x_train[random_indices[i]])\n",
    "    axs[i].set_title('The label: ' + str(actual_label_list_train[random_indices[i]]))\n",
    "    axs[i].axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jGCSVkGc3ZCs",
    "outputId": "a7962f19-09b1-4bc2-c9c5-52fb14ef1544"
   },
   "outputs": [],
   "source": [
    "#reshaping the dataset\n",
    "\n",
    "x_train_scaled = x_train.reshape(np.shape(x_train)[0], np.shape(x_train)[1]*np.shape(x_train)[2]*3)/255\n",
    "x_test_scaled = x_test.reshape(np.shape(x_test)[0], np.shape(x_test)[1]*np.shape(x_test)[2]*3)/255\n",
    "\n",
    "#transforming to categorical values\n",
    "\n",
    "y_train_oh = keras.utils.to_categorical(y_train)\n",
    "y_test_oh = keras.utils.to_categorical(y_test)\n",
    "\n",
    "#let's check out the new shapes!\n",
    "\n",
    "print('The shape of the train dataset: ', np.shape(x_train_scaled))\n",
    "print('The shape of the test dataset: ', np.shape(x_test_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MNKPgVc-3ZCs"
   },
   "outputs": [],
   "source": [
    "#calling the logistic regression function\n",
    "\n",
    "clf = LogisticRegression(multi_class='multinomial', random_state=0, max_iter=800, n_jobs=-1).fit(x_train_scaled, y_train.T[0])\n",
    "\n",
    "#predicting the data\n",
    "\n",
    "predicted_test = clf.predict(x_test_scaled)\n",
    "confusion_matrix_values = confusion_matrix(y_test.T[0], predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "MtdtDmqKCSuT",
    "outputId": "2ed44c86-b555-4852-f02c-684b27ba4c9a"
   },
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "\n",
    "plt.subplots(figsize=(10,7))\n",
    "\n",
    "p1 = sns.heatmap(confusion_matrix_values, xticklabels=actual_labels, yticklabels=actual_labels, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "plt.ylabel('Real values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Heatmap of the real and predicted values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHrE1vLcG2-e"
   },
   "source": [
    "As it's seen, the dogs and cats are quite similar, according to the calssifier. The ships and airplanes are also quite indistinguishable, just like the truck and automobiles. These are not suprising results. This might cause the acccuracy to be unsufficiently low. Let's check it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yq8E6gqpDdCW",
    "outputId": "bd400a9a-d172-4f32-8095-f1f633cd807f"
   },
   "outputs": [],
   "source": [
    "#calculating the accuracy\n",
    "\n",
    "acc = accuracy_score(y_test, predicted_test)\n",
    "\n",
    "print('The accuracy of the model is:', round(acc,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMKd2AmvEctA"
   },
   "source": [
    "As it's seen, one logistic regression model doesn't result in very high accuracy. Let's try another apporoach, which is fitting logistic regressions to all the different labels and then check the accuracy! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yieFDxooCqIT"
   },
   "outputs": [],
   "source": [
    "#creating the one vs rest classifier model\n",
    "\n",
    "clf2 = OneVsRestClassifier(LogisticRegression(multi_class='multinomial', random_state=0, max_iter=200, n_jobs=-1))\n",
    "\n",
    "y_score = clf2.fit(x_train_scaled, y_train_oh).decision_function(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Ag9lZ_0I2kd"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = []\n",
    "for i in range(len(y_score[0])):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_oh[:, i], y_score[:, i])\n",
    "for i in range(len(y_score[0])):\n",
    "    roc_auc.append(round(auc_score(fpr[i], tpr[i]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "IPPcH0rAJB9t",
    "outputId": "621bb5fb-784e-49df-a185-8fc762e6fba2"
   },
   "outputs": [],
   "source": [
    "#plotting the results:\n",
    "\n",
    "colours = ['orange', 'gold', 'firebrick', 'lightblue', 'lightgreen', 'pink', 'brown', 'forestgreen', 'gray', 'purple']\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(y_score[0])):\n",
    "\n",
    "    plt.plot(fpr[i], tpr[i], color = colours[i], label='' + f': {actual_labels[i]}' + ' AUC' + f': {roc_auc[i]}')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--', c='k')\n",
    "plt.xlabel('False Positive Rate', fontsize = 12)\n",
    "plt.ylabel('True Positive Rate', fontsize = 12)\n",
    "plt.grid()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D5P04a7z3ZCt"
   },
   "source": [
    "### 2. Train an `SGDClassifier` regression model on the dataset and predict all the test outcomes with the `sklearn` API. \n",
    "\n",
    "* Select an appropiate loss for this task, explain what this means.\n",
    "* Time is precious, run the classifier paralell on many jobs.\n",
    "* Plot the ROC curves and AUC scores on the same figure for the test set.\n",
    "* Calculate the accuracy of the classifier.\n",
    "* Describe the above model with your own words, how is it different than the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VRhnPaNFNDMd"
   },
   "source": [
    "Modified huber loss funciton is mainly used in robust regressions. It's less sensitive to outliers than the squared error loss. Even though this is a classification model, this loss function proved to be quite effective when solving this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BQ1fW7Hp3ZCt"
   },
   "outputs": [],
   "source": [
    "#creating the SGD model\n",
    "\n",
    "sgd = SGDClassifier(loss=\"modified_huber\", max_iter=800, n_jobs=-1)\n",
    "\n",
    "sgd.fit(x_train_scaled, y_train.T[0])\n",
    "\n",
    "#predicting the data\n",
    "\n",
    "predicted_test_sgd = sgd.predict(x_test_scaled)\n",
    "confusion_matrix_values_sgd = confusion_matrix(y_test.T[0], predicted_test_sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "d8QoSOAib6MX",
    "outputId": "56859734-366b-4813-a7cd-0ee5d6bdc176"
   },
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "\n",
    "plt.subplots(figsize=(10,7))\n",
    "\n",
    "p1 = sns.heatmap(confusion_matrix_values_sgd, xticklabels=actual_labels, yticklabels=actual_labels, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "plt.ylabel('Real values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Heatmap of the real and predicted values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0GO_tyaV0dJ",
    "outputId": "d0ee52ec-6e73-4258-9807-5814576a2011"
   },
   "outputs": [],
   "source": [
    "#calculating the accuracy\n",
    "\n",
    "acc_sgd = accuracy_score(y_test, predicted_test_sgd)\n",
    "\n",
    "print('The accuracy of the model is:', round(acc_sgd,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MiX712aZcHPX"
   },
   "outputs": [],
   "source": [
    "#creating the one vs rest classifier model\n",
    "\n",
    "sgd2 = OneVsRestClassifier(SGDClassifier(loss=\"modified_huber\", max_iter=800, n_jobs=-1))\n",
    "\n",
    "y_score_sgd = sgd2.fit(x_train_scaled, y_train_oh).decision_function(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bNN6r5qXTtIs"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr_sgd = dict()\n",
    "tpr_sgd = dict()\n",
    "roc_auc_sgd = []\n",
    "for i in range(len(y_score_sgd[0])):\n",
    "    fpr_sgd[i], tpr_sgd[i], _ = roc_curve(y_test_oh[:, i], y_score_sgd[:, i])\n",
    "for i in range(len(y_score_sgd[0])):\n",
    "    roc_auc_sgd.append(round(auc_score(fpr_sgd[i], tpr_sgd[i]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "FED3YfQ83ZCt",
    "outputId": "8b40fed5-dfa1-4776-c8a0-eaaab912e8ba"
   },
   "outputs": [],
   "source": [
    "#plotting the results:\n",
    "\n",
    "colours = ['orange', 'gold', 'firebrick', 'lightblue', 'lightgreen', 'pink', 'brown', 'forestgreen', 'gray', 'purple']\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(y_score_sgd[0])):\n",
    "\n",
    "    plt.plot(fpr_sgd[i], tpr_sgd[i], color = colours[i], label='' + f': {actual_labels[i]}' + ' AUC' + f': {roc_auc_sgd[i]}')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--', c='k')\n",
    "plt.xlabel('False Positive Rate', fontsize = 12)\n",
    "plt.ylabel('True Positive Rate', fontsize = 12)\n",
    "plt.grid()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3H8tseeOR-Z"
   },
   "source": [
    "The SGD classifier uses Stochastic Gradient Descent as a solver. As far as I know, the SGD method is an optimization method, while Logistic Regression is a machine learning algorithm. SGDClassifier is a linear classifier using SGD optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3UAscEB3ZCt"
   },
   "source": [
    "### 3. Train a RandomForest classifier\n",
    "\n",
    "* Plot the ROC curve with AUC scores on the test set.\n",
    "* Calculate accuracy of the classifier on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tPCNjZbPMnFI"
   },
   "outputs": [],
   "source": [
    "#creating the model\n",
    "\n",
    "rf = RandomForestClassifier(random_state = 42)\n",
    "rf.fit(x_train_scaled, y_train.T[0])\n",
    "\n",
    "#predicting the data\n",
    "\n",
    "predicted_test_rf = rf.predict(x_test_scaled)\n",
    "confusion_matrix_values_rf = confusion_matrix(y_test.T[0], predicted_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "H6qYeF9PeM8Y",
    "outputId": "66bf64e9-ed8e-46ea-b56b-de3c121990ce"
   },
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "\n",
    "plt.subplots(figsize=(10,7))\n",
    "\n",
    "p1 = sns.heatmap(confusion_matrix_values_rf, xticklabels=actual_labels, yticklabels=actual_labels, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "plt.ylabel('Real values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Heatmap of the real and predicted values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A04_INjgPPA8",
    "outputId": "b101bdc0-bf80-44e1-dd22-169ea26b803f"
   },
   "outputs": [],
   "source": [
    "#calculating the accuracy\n",
    "\n",
    "acc_rf = accuracy_score(y_test, predicted_test_rf)\n",
    "\n",
    "print('The accuracy of the model is:', round(acc_rf,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTbKsdDVv-M8"
   },
   "outputs": [],
   "source": [
    "#creating the one vs rest classifier model\n",
    "\n",
    "rf2 = OneVsRestClassifier(RandomForestClassifier(random_state = 0, n_jobs=-1))\n",
    "\n",
    "rf2.fit(x_train_scaled, y_train_oh)\n",
    "\n",
    "y_score_rf2 = rf2.predict_proba(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "toOM4_fYnBSI"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr_rf = dict()\n",
    "tpr_rf = dict()\n",
    "roc_auc_rf = []\n",
    "for i in range(len(y_score_rf2[0])):\n",
    "    fpr_rf[i], tpr_rf[i], _ = roc_curve(y_test_oh[:, i], y_score_rf2[:, i])\n",
    "for i in range(len(y_score_rf2[0])):\n",
    "    roc_auc_rf.append(round(auc_score(fpr_rf[i], tpr_rf[i]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "loA2FaUpF0JY",
    "outputId": "cd6e66b1-ccc3-4950-916b-bacea092eaa5"
   },
   "outputs": [],
   "source": [
    "#plotting the results:\n",
    "\n",
    "colours = ['orange', 'gold', 'firebrick', 'lightblue', 'lightgreen', 'pink', 'brown', 'forestgreen', 'gray', 'purple']\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(y_score_rf2[0])):\n",
    "\n",
    "    plt.plot(fpr_rf[i], tpr_rf[i], color = colours[i], label='' + f': {actual_labels[i]}' + ' AUC' + f': {roc_auc_rf[i]}')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--', c='k')\n",
    "plt.xlabel('False Positive Rate', fontsize = 12)\n",
    "plt.ylabel('True Positive Rate', fontsize = 12)\n",
    "plt.grid()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pH5f2b0X3ZCt"
   },
   "source": [
    "### 4. Train an multi layer perceptron classifier\n",
    "\n",
    "* use the `MLPClassifier` from `sklearn`\n",
    "* Set its parameter to `max_iter = 30` or if you have time, set it for at least `100`. After `30` iterations the model does not converge but gives reasonable predictions (with default parameters).\n",
    "* Plot the ROC curves with AUC scores for the test set.\n",
    "* Calculate the accuracy of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruKES0N0PzI-",
    "outputId": "277ba3a1-d9aa-43ac-b193-e7c78a326cb0"
   },
   "outputs": [],
   "source": [
    "#creating the model and fitting the data\n",
    "\n",
    "mlpc = MLPClassifier(random_state=1, max_iter=30).fit(x_train_scaled, y_train.T[0])\n",
    "\n",
    "#predicting the results\n",
    "\n",
    "y_test_predicted_mlpc = mlpc.predict(x_test_scaled)\n",
    "confusion_matrix_values_mlpc = confusion_matrix(y_test.T[0], y_test_predicted_mlpc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 507
    },
    "id": "n1YY188akSKs",
    "outputId": "ef63474a-4883-437b-84c6-416596d9459b"
   },
   "outputs": [],
   "source": [
    "#plotting the confusion matrix\n",
    "\n",
    "plt.subplots(figsize=(10,7))\n",
    "\n",
    "p1 = sns.heatmap(confusion_matrix_values_mlpc, xticklabels=actual_labels, yticklabels=actual_labels, annot=True, fmt=\"d\", cmap='Blues')\n",
    "\n",
    "plt.ylabel('Real values')\n",
    "plt.xlabel('Predicted values')\n",
    "plt.title('Heatmap of the real and predicted values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRmAUhNqPzPW",
    "outputId": "93ab1031-e310-46c0-f57f-677a752d8fe6"
   },
   "outputs": [],
   "source": [
    "#calculating the accuracy\n",
    "\n",
    "acc_mlpc = accuracy_score(y_test, y_test_predicted_mlpc)\n",
    "\n",
    "print('The accuracy of the model:', round(acc_mlpc,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8D8nfmi-ERPc",
    "outputId": "2a4c5a67-03eb-418f-cecc-5c795b4f14bc"
   },
   "outputs": [],
   "source": [
    "#creating the one vs rest classifier model\n",
    "\n",
    "mlpc2 = OneVsRestClassifier(MLPClassifier(random_state=1, max_iter=30))\n",
    "\n",
    "mlpc2.fit(x_train_scaled, y_train_oh)\n",
    "\n",
    "y_score_mlpc = mlpc2.predict_proba(x_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48ZM9BpvPzSD"
   },
   "outputs": [],
   "source": [
    "# Compute ROC curve and ROC area for each class\n",
    "\n",
    "fpr_mlpc = dict()\n",
    "tpr_mlpc = dict()\n",
    "roc_auc_mlpc = []\n",
    "for i in range(len(y_score_mlpc[0])):\n",
    "    fpr_mlpc[i], tpr_mlpc[i], _ = roc_curve(y_test_oh[:, i], y_score_mlpc[:, i])\n",
    "for i in range(len(y_score_mlpc[0])):\n",
    "    roc_auc_mlpc.append(round(auc_score(fpr_mlpc[i], tpr_mlpc[i]),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "nb2qOFj8D-Jr",
    "outputId": "3bfc79b3-68c7-4677-deb7-d60eeeb67dad"
   },
   "outputs": [],
   "source": [
    "#plotting the results:\n",
    "\n",
    "colours = ['orange', 'gold', 'firebrick', 'lightblue', 'lightgreen', 'pink', 'brown', 'forestgreen', 'gray', 'purple']\n",
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "for i in range(len(y_score_mlpc[0])):\n",
    "\n",
    "    plt.plot(fpr_mlpc[i], tpr_mlpc[i], color = colours[i], label='' + f': {actual_labels[i]}' + ' AUC' + f': {roc_auc_mlpc[i]}')\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], '--', c='k')\n",
    "plt.xlabel('False Positive Rate', fontsize = 12)\n",
    "plt.ylabel('True Positive Rate', fontsize = 12)\n",
    "plt.grid()\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjPxWYGP3ZCt"
   },
   "source": [
    "### 5. Train a ResNet50 CNN model on the dataset, utilize ImageNet pre-trained weights and fine-tune for at least 3 epochs:\n",
    "\n",
    "* training for 3 epochs should be enough to prove that this model is superior compared to others, train longer to explore the possibilities of the model\n",
    "\n",
    "Convert the dataset:\n",
    "\n",
    "```python\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(32)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
    "test_dataset = test_dataset.batch(32)\n",
    "```\n",
    "\n",
    "Hints:\n",
    "\n",
    "* loading a pretrained model and letting its parameters be tunable\n",
    "\n",
    "```python\n",
    "backbone = tf.keras.applications.YOUR_MODEL_OF_CHOICE # set include_top = False to get rid of the dense layers\n",
    "backbone.trainable = True # set if you want to fine-tune the pretrained weights too, otherwise set to False\n",
    "```\n",
    "\n",
    "* defining your custom model with the pretrained backbone\n",
    "\n",
    "```python\n",
    "# YOUR_MODEL_OF_CHOICE here is ResNet50 as per the task description.\n",
    "\n",
    "# Functional TensorFlow API\n",
    "def my_own_model():\n",
    "  inp = tf.keras.layers.Input(shape=(32, 32, 3))\n",
    "  x = tf.keras.applications.YOUR_MODEL_OF_CHOISE.preprocess_input(inp)\n",
    "\n",
    "  x = backbone(x)\n",
    "  # Here comes some more layers\n",
    "  # and flattening where needed!\n",
    "  out = # layer outputting the specified number of classes\n",
    "        # with or without a softmax activation, later on\n",
    "        # the choice of the loss depends on this\n",
    "  model = tf.keras.models.Model(inputs=inp, outputs=out)\n",
    "  return model\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNxwd3T_3ZCt"
   },
   "outputs": [],
   "source": [
    "#creating the preprocessing function for the images\n",
    "\n",
    "def imagenet_convert(img):\n",
    "    img  = img.astype(float)[...,::-1] # RGB --> BGR\n",
    "    img -= [103.939, 116.779, 123.68]\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BF4IOqf-01m_"
   },
   "outputs": [],
   "source": [
    "#loading the pre-trained model\n",
    "\n",
    "model = tensorflow.keras.applications.ResNet50(weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy3cdmrp1B0Z"
   },
   "outputs": [],
   "source": [
    "#loading the cifar10 dataset \n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "#converting the images\n",
    "\n",
    "x_train_imagenet = np.array([imagenet_convert(x) for x in x_train])\n",
    "x_test_imagenet = np.array([imagenet_convert(x) for x in x_test])\n",
    "\n",
    "y_train_oh = keras.utils.to_categorical(y_train.T[0])\n",
    "y_test_oh = keras.utils.to_categorical(y_test.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlXJ3Veo0H4d"
   },
   "outputs": [],
   "source": [
    "#we shall pop the last layer, since we need a 10 long dense layer in the cifar10 dataset\n",
    "\n",
    "model._layers.pop()\n",
    "inputs = model.input\n",
    "output = model.layers[-1].output\n",
    "output = keras.layers.Dense(10, activation = 'softmax')(output)\n",
    "model = keras.models.Model(inputs, output)\n",
    "\n",
    "#we need to freeze the layers, except for the last one\n",
    "\n",
    "for i in model.layers[:-1]:\n",
    "    i.trainable = False\n",
    "\n",
    "#let's compile the model\n",
    "\n",
    "model.compile(optimizer = keras.optimizers.Adam(lr = 1e-4),loss = 'categorical_crossentropy',metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "baC-lMhM2vH_"
   },
   "outputs": [],
   "source": [
    "#let's check out the parameters!\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7al2_zcS3NbZ"
   },
   "source": [
    "As it's seen, we froze all the layers, except for the last one. This means that all those parameters are non-trainable and only 20.600 are trainable. Let's fit the model and check out the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rolD5AEs0H7H"
   },
   "outputs": [],
   "source": [
    "#training the model\n",
    "\n",
    "history = model.fit(x_train_imagenet, y_train_oh, batch_size = 32, epochs = 3, validation_data = (x_test_imagenet, y_test_oh), shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-8yPJlUK-qZ4"
   },
   "source": [
    "As it's seen, the validation accuracy is around 60-77% (depends on the run). This is almost a 10-20% increase from even the best machine learning technique. Not only is this a significant improvement in accuarcy, but also in computing time. The machine learning techniques took about 10-60 minutes, while the neural network required only approximately 3 minutes. This means that using neural networks for this problem proved to be much more efficient. "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "hw11 (2).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
